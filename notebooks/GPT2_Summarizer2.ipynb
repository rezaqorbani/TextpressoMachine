{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import GPT2LMHeadModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class XSumPreprocessor:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Add special tokens to the tokenizer\n",
    "        self.special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'sep_token': '<sep>', 'pad_token': '<pad>'}\n",
    "        self.num_added_toks = self.tokenizer.add_special_tokens(self.special_tokens_dict)\n",
    "    def preprocess(self, example):\n",
    "        # Concatenate article and summary and add special tokens\n",
    "        encoded_example = tokenizer.encode_plus(\n",
    "            f'{self.special_tokens_dict[\"bos_token\"]} {example[\"document\"]} {self.special_tokens_dict[\"sep_token\"]} {example[\"summary\"]} {self.special_tokens_dict[\"eos_token\"]}',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return encoded_example\n",
    "\n",
    "    def filter(self, dataset):\n",
    "        dataset = [sample for sample in dataset if self.tokenizer.sep_token_id in sample['input_ids']]\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f4adc9dec74104b51c0b40cec3f7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "max_length=1024\n",
    "\n",
    "preprocessor = XSumPreprocessor(\n",
    "                tokenizer = tokenizer,\n",
    "                max_length = max_length\n",
    ")\n",
    "\n",
    "# Load XSum dataset\n",
    "xsum_dataset = load_dataset('xsum')\n",
    "\n",
    "use_percent = 1\n",
    "dataset_train = load_dataset(\"xsum\", split=f\"train[:{use_percent}%]\")\n",
    "dataset_val = load_dataset(\"xsum\", split=f\"validation[:{use_percent}%]\")\n",
    "dataset_test = load_dataset(\"xsum\", split=f\"test[:{use_percent}%]\")\n",
    "\n",
    "dataset = DatasetDict({'train': dataset_train, 'validation': dataset_val, 'test': dataset_test})\n",
    "\n",
    "# Apply the function to all examples in the dataset\n",
    "xsum_dataset = dataset.map(preprocessor.preprocess, remove_columns=['document', 'summary'])\n",
    "# Format the dataset to PyTorch tensors and split into training, validation, and test sets\n",
    "xsum_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "train_dataset = preprocessor.filter(xsum_dataset['train'])\n",
    "val_dataset = preprocessor.filter(xsum_dataset['validation'])\n",
    "test_dataset = xsum_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "### TODO: Save losses while training ###\n",
    "### TODO: Add Checkpointing ###\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "class GPT2FineTuner(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model#GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        # Resize token embeddings in case you have added more tokens in the vocab\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss only on the reference summary\n",
    "        loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1]  # Use the single element from the tensor\n",
    "\n",
    "            shift_logits = logits[i, sep_position:-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            loss += cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss = loss / len(sep_positions)  # average loss\n",
    "\n",
    "        return {'loss': loss}\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "       # Calculate loss only on the reference summary\n",
    "        val_loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1] # Use the single element from the tensor\n",
    "            shift_logits = logits[i, sep_position:-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            val_loss += cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        val_loss = val_loss / len(sep_positions)  # average loss\n",
    "\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, batch_size=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path):\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "        return cls(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M \n",
      "------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.772   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5210cfdc4f2a43468046900345cdfc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = GPT2FineTuner(model)\n",
    "\n",
    "trainer = Trainer(max_epochs=3, accumulate_grad_batches=4)\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2FineTuner' object has no attribute 'training_losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/399637490.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2FineTuner' object has no attribute 'training_losses'"
     ]
    }
   ],
   "source": [
    "### NB: Will workd once losses are saved ###\n",
    "print('training loss', model.training_losses)\n",
    "print('validation loss', model.validation_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model.model.save_pretrained('./saved/models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('./saved/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_summarizer = GPT2FineTuner(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(model, text, length, device):\n",
    "    ## From the blog ##\n",
    "    text = torch.tensor(text, dtype=torch.long, device=device)\n",
    "    text = text.unsqueeze(0)\n",
    "    generated = text\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in tnrange(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs[0][0, -1, :]\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer =  GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'sep_token': '<sep>', 'pad_token': '<pad>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=test_dataset[10]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sep_idx=(example == tokenizer.sep_token_id).nonzero(as_tuple=False).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(sep_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88/1770653979.py:8: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879fbe9c7da14f4092370c313da1e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = example[:sep_idx].tolist()\n",
    "summary =example[sep_idx+1:].tolist()\n",
    "generated_text = summarize(gpt2_summarizer, text, length=200, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_summary = generated_text[0, sep_idx:].tolist()\n",
    "id_summary = tokenizer.convert_ids_to_tokens(tokenized_summary,skip_special_tokens=True)\n",
    "gpt2_summary = tokenizer.convert_tokens_to_string(id_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Original Text #############\n",
      "<bos> Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\n",
      "Detectives said three firearms, ammunition and a five-figure sum of money were recovered.\n",
      "A 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court on Thursday.\n",
      "\n",
      "######### GPT2 Summary ##############\n",
      "\n",
      "\n",
      "######### Ground Truth Summary ###########\n",
      "A man has appeared in court after firearms, ammunition and cash were seized by police in Edinburgh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('######### Original Text #############')\n",
    "print(tokenizer.decode(text), end='\\n\\n')\n",
    "print('######### GPT2 Summary ##############')\n",
    "print(gpt2_summary, end='\\n\\n')\n",
    "print('######### Ground Truth Summary ###########')\n",
    "print(tokenizer.decode(summary, skip_special_tokens=True), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
