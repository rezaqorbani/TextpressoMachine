{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import GPT2LMHeadModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class XSumPreprocessor:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Add special tokens to the tokenizer\n",
    "        self.special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'sep_token': '<sep>', 'pad_token': '<pad>'}\n",
    "        self.num_added_toks = self.tokenizer.add_special_tokens(self.special_tokens_dict)\n",
    "    def preprocess(self, example):\n",
    "        # Concatenate article and summary and add special tokens\n",
    "        encoded_example = tokenizer.encode_plus(\n",
    "            f'{self.special_tokens_dict[\"bos_token\"]} {example[\"document\"]} {self.special_tokens_dict[\"sep_token\"]} {example[\"summary\"]} {self.special_tokens_dict[\"eos_token\"]}',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return encoded_example\n",
    "\n",
    "    def filter(self, dataset):\n",
    "        dataset = [sample for sample in dataset if self.tokenizer.sep_token_id in sample['input_ids']]\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aa97726a5341ab93a3cbd7b7ba7ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "max_length=1024\n",
    "\n",
    "preprocessor = XSumPreprocessor(\n",
    "                tokenizer = tokenizer,\n",
    "                max_length = max_length\n",
    ")\n",
    "\n",
    "# Load XSum dataset\n",
    "xsum_dataset = load_dataset('xsum')\n",
    "\n",
    "use_percent = 5\n",
    "dataset_train = load_dataset(\"xsum\", split=f\"train[:{use_percent}%]\")\n",
    "dataset_val = load_dataset(\"xsum\", split=f\"validation[:{use_percent}%]\")\n",
    "dataset_test = load_dataset(\"xsum\", split=f\"test[:{use_percent}%]\")\n",
    "\n",
    "dataset = DatasetDict({'train': dataset_train, 'validation': dataset_val, 'test': dataset_test})\n",
    "\n",
    "# Apply the function to all examples in the dataset\n",
    "xsum_dataset = dataset.map(preprocessor.preprocess, remove_columns=['document', 'summary'])\n",
    "# Format the dataset to PyTorch tensors and split into training, validation, and test sets\n",
    "xsum_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "train_dataset = preprocessor.filter(xsum_dataset['train'])\n",
    "val_dataset = preprocessor.filter(xsum_dataset['validation'])\n",
    "test_dataset = xsum_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "### TODO: Save losses while training ###\n",
    "### TODO: Add Checkpointing ###\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "class GPT2FineTuner(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        # Resize token embeddings in case you have added more tokens in the vocab\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss only on the reference summary\n",
    "        loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1]  # Use the single element from the tensor\n",
    "\n",
    "            shift_logits = logits[i, sep_position:-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            loss += cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss = loss / len(sep_positions)  # average loss\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "       # Calculate loss only on the reference summary\n",
    "        val_loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1] # Use the single element from the tensor\n",
    "            shift_logits = logits[i, sep_position:-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            val_loss += cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        val_loss = val_loss / len(sep_positions)  # average loss\n",
    "        self.validation_losses.append(val_loss.item())\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, batch_size=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/studio-lab-user/sagemaker-studiolab-notebooks/TextPressoMachine/saved/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M \n",
      "------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.772   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab52ac639bd427293124cb84c3ccde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = GPT2FineTuner()\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Choose your metric here\n",
    "    dirpath='./saved/models/',\n",
    "    filename='GPT2FineTuner-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',  # or 'max', depending on what you want to monitor\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=5, accumulate_grad_batches=4, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch0: val_loss 0.167\n",
    "epoch1: val_loss 0.163\n",
    "epoch2: val_loss 0.165\n",
    "epoch3: val_loss 0.174\n",
    "epoch4: val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### NB: Will workd once losses are saved ###\n",
    "#print('training loss', model.train_losses)\n",
    "#print('validation loss', model.validation_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved/tokenizers/tokenizer_config.json',\n",
       " './saved/tokenizers/special_tokens_map.json',\n",
       " './saved/tokenizers/vocab.json',\n",
       " './saved/tokenizers/merges.txt',\n",
       " './saved/tokenizers/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model\n",
    "tokenizer.save_pretrained(\"./saved/tokenizers/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model is saved at the path:\n",
    "checkpoint_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2FineTuner(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50261, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2FineTuner.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TextPressoMachine/saved/models/GPT2FineTuner-epoch=01-val_loss=0.16.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./saved/tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_summarizer = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(model, text, length, device):\n",
    "    ## From the blog ##\n",
    "    text = torch.tensor(text, dtype=torch.long, device=device)\n",
    "    text = text.unsqueeze(0)\n",
    "    generated = text\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in tnrange(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs[0][0, -1, :]\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=test_dataset[25]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sep_idx=(example == tokenizer.sep_token_id).nonzero(as_tuple=False).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    }
   ],
   "source": [
    "print(sep_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92/1770653979.py:8: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3813796a0f4d778089acf531f30c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = example[:sep_idx].tolist()\n",
    "summary =example[sep_idx+1:].tolist()\n",
    "generated_text = summarize(gpt2_summarizer, text, length=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_summary = generated_text[0, sep_idx:].tolist()\n",
    "id_summary = tokenizer.convert_ids_to_tokens(tokenized_summary,skip_special_tokens=True)\n",
    "gpt2_summary = tokenizer.convert_tokens_to_string(id_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Original Text #############\n",
      "<bos> The fine follows the conviction of former RBS trader, Shirlina Tsang, for fraud last year.\n",
      "She was sentenced to 50 months in prison after being caught falsifying records of emerging markets trades.\n",
      "Hong Kong regulators said RBS's controls were \"seriously inadequate\".\n",
      "The Securities and Futures Commission (SFC) also said there were \"significant weaknesses in its procedures, management systems and internal controls.\"\n",
      "But the regulator said the fine took into account the bank's speedy action in alerting the authorities once it had discovered the illegal trades, which took place in its emerging markets rates business in 2011.\n",
      "\"This deserves substantial credit and is the reason why today's sanctions are not heavier ones,\" Mark Steward, the SFC's head of enforcement, said in a statement.\n",
      "RBS responded with a statement, reading: \"We put in place a comprehensive remediation programme that strengthened our governance and supervisory oversight, and our control environment.\"\n",
      "The fine is relatively small compared to others the bank has received in the last few years.\n",
      "In December RBS agreed to pay 391m euros (£320m) in penalties to the European Commission for its role in the attempted rigging of Yen Libor and Euribor - the Tokyo and euro equivalents of the London interbank offered rate, or Libor.\n",
      "In the same month it was fined $100m (£60m) by US regulators for violations of US sanctions against Iran, Sudan, Burma, and Cuba.\n",
      "The bank was found to have removed location information on payments made to US financial institutions from countries such as Iran and Cuba.\n",
      "\n",
      "######### GPT2 Summary ##############\n",
      " Open Bank European Union has fined RBS $2.7bn (£2.8bn) for gun-running offences, according to a Wall Street Journal investigation.\n",
      "\n",
      "######### Ground Truth Summary ###########\n",
      "Royal Bank of Scotland (RBS) has been fined HK$6m (£460,000) by Hong Kong regulators after it failed to detect a series of unauthorised transactions by one of its traders.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('######### Original Text #############')\n",
    "print(tokenizer.decode(text), end='\\n\\n')\n",
    "print('######### GPT2 Summary ##############')\n",
    "print(gpt2_summary, end='\\n\\n')\n",
    "print('######### Ground Truth Summary ###########')\n",
    "print(tokenizer.decode(summary, skip_special_tokens=True), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
