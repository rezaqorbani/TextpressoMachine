{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import GPT2LMHeadModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from tqdm import tnrange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class XSumPreprocessor:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Add special tokens to the tokenizer\n",
    "        self.special_tokens_dict = {'bos_token': '<bos>', 'eos_token': '<eos>', 'sep_token': '<sep>', 'pad_token': '<pad>'}\n",
    "        self.num_added_toks = self.tokenizer.add_special_tokens(self.special_tokens_dict)\n",
    "    def preprocess(self, example):\n",
    "        # Concatenate article and summary and add special tokens\n",
    "        encoded_example = tokenizer.encode_plus(\n",
    "            f'{self.special_tokens_dict[\"bos_token\"]} {example[\"document\"]} {self.special_tokens_dict[\"sep_token\"]} {example[\"summary\"]} {self.special_tokens_dict[\"eos_token\"]}',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return encoded_example\n",
    "\n",
    "    def filter(self, dataset):\n",
    "        dataset = [sample for sample in dataset if self.tokenizer.sep_token_id in sample['input_ids']]\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c97bfc02b4b63b97a4a9c73a693bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Found cached dataset xsum (/home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71/cache-68165d29daa15d20.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71/cache-011bd28cfb144aed.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71/cache-bf89ead219ca13fd.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "max_length=1024\n",
    "\n",
    "preprocessor = XSumPreprocessor(\n",
    "                tokenizer = tokenizer,\n",
    "                max_length = max_length\n",
    ")\n",
    "\n",
    "# Load XSum dataset\n",
    "xsum_dataset = load_dataset('xsum')\n",
    "\n",
    "use_percent = 1\n",
    "dataset_train = load_dataset(\"xsum\", split=f\"train[:{use_percent}%]\")\n",
    "dataset_val = load_dataset(\"xsum\", split=f\"validation[:{use_percent*2}%]\")\n",
    "dataset_test = load_dataset(\"xsum\", split=f\"test[:{use_percent}%]\")\n",
    "\n",
    "dataset = DatasetDict({'train': dataset_train, 'validation': dataset_val, 'test': dataset_test})\n",
    "\n",
    "# Apply the function to all examples in the dataset\n",
    "xsum_dataset = dataset.map(preprocessor.preprocess, remove_columns=['document', 'summary'])\n",
    "# Format the dataset to PyTorch tensors and split into training, validation, and test sets\n",
    "xsum_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "train_dataset = preprocessor.filter(xsum_dataset['train'])\n",
    "val_dataset = preprocessor.filter(xsum_dataset['validation'])\n",
    "test_dataset = xsum_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TODO: Save losses while training ###\n",
    "### TODO: Add Checkpointing ###\n",
    "\n",
    "\n",
    "class GPT2FineTuner(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        # Resize token embeddings in case you have added more tokens in the vocab\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss only on the reference summary\n",
    "        loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1]  # Use the single element from the tensor\n",
    "            shift_logits = logits[i, sep_position:-1,:].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            loss_calculation = cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss += loss_calculation if not torch.isnan(loss_calculation) else 0\n",
    "\n",
    "        loss = loss / len(sep_positions)  # average loss\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask = batch['input_ids'], batch['attention_mask']\n",
    "        sep_positions = (input_ids == tokenizer.sep_token_id).nonzero(as_tuple=False)\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "       # Calculate loss only on the reference summary\n",
    "        val_loss = 0\n",
    "        for i, sep_position in enumerate(sep_positions):\n",
    "            sep_position = sep_position[1] # Use the single element from the tensor\n",
    "            shift_logits = logits[i, sep_position:-1, :].contiguous()\n",
    "            shift_labels = input_ids[i, sep_position+1:].contiguous()\n",
    "            val_loss += cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        val_loss = val_loss / len(sep_positions)  # average loss\n",
    "        self.validation_losses.append(val_loss.item())\n",
    "        self.log('val_loss', val_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, batch_size=2, num_workers=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/studio-lab-user/dev/TextpressoMachine/notebooks exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M \n",
      "------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.772   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bec1c8aad04f0a9b12ddd0c4c9d4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a36f9c3d504008a09212f2752007b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2FineTuner()\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Choose your metric here\n",
    "    dirpath='./',\n",
    "    filename='GPT2FineTuner-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',  # or 'max', depending on what you want to monitor\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=2, accumulate_grad_batches=4, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model)\n",
    "\n",
    "### NB: Will workd once losses are saved ###\n",
    "#print('training loss', model.train_losses)\n",
    "#print('validation loss', model.validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch0: val_loss 0.167\n",
    "epoch1: val_loss 0.163\n",
    "epoch2: val_loss 0.165\n",
    "epoch3: val_loss 0.174\n",
    "epoch4: val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./\")\n",
    "\n",
    "# The best model is saved at the path:\n",
    "checkpoint_path = checkpoint_callback.best_model_path\n",
    "model = GPT2FineTuner.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "gpt2_summarizer = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(model, text, length, device):\n",
    "    ## From the blog ##\n",
    "    text = torch.tensor(text, dtype=torch.long, device=device)\n",
    "    text = text.unsqueeze(0)\n",
    "    generated = text\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in tnrange(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            next_token_logits = outputs[0][0, -1, :]\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./\")\n",
    "example=test_dataset[25]['input_ids']\n",
    "sep_idx=(example == tokenizer.sep_token_id).nonzero(as_tuple=False).item()\n",
    "print(sep_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text = example[:sep_idx].tolist()\n",
    "summary =example[sep_idx+1:].tolist()\n",
    "generated_text = summarize(gpt2_summarizer, text, length=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_summary = generated_text[0, sep_idx:].tolist()\n",
    "id_summary = tokenizer.convert_ids_to_tokens(tokenized_summary,skip_special_tokens=True)\n",
    "gpt2_summary = tokenizer.convert_tokens_to_string(id_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('######### Original Text #############')\n",
    "print(tokenizer.decode(text), end='\\n\\n')\n",
    "print('######### GPT2 Summary ##############')\n",
    "print(gpt2_summary, end='\\n\\n')\n",
    "print('######### Ground Truth Summary ###########')\n",
    "print(tokenizer.decode(summary, skip_special_tokens=True), end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
